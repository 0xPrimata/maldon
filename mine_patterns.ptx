//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-34714021
// Cuda compilation tools, release 12.6, V12.6.68
// Based on NVVM 7.0.1
//

.version 8.5
.target sm_75
.address_size 64

	// .globl	mine_patterns
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.global .align 4 .b8 keccak_rho_offsets[96] = {1, 0, 0, 0, 3, 0, 0, 0, 6, 0, 0, 0, 10, 0, 0, 0, 15, 0, 0, 0, 21, 0, 0, 0, 28, 0, 0, 0, 36, 0, 0, 0, 45, 0, 0, 0, 55, 0, 0, 0, 2, 0, 0, 0, 14, 0, 0, 0, 27, 0, 0, 0, 41, 0, 0, 0, 56, 0, 0, 0, 8, 0, 0, 0, 25, 0, 0, 0, 43, 0, 0, 0, 62, 0, 0, 0, 18, 0, 0, 0, 39, 0, 0, 0, 61, 0, 0, 0, 20, 0, 0, 0, 44};
.global .align 4 .b8 keccak_pi[96] = {10, 0, 0, 0, 7, 0, 0, 0, 11, 0, 0, 0, 17, 0, 0, 0, 18, 0, 0, 0, 3, 0, 0, 0, 5, 0, 0, 0, 16, 0, 0, 0, 8, 0, 0, 0, 21, 0, 0, 0, 24, 0, 0, 0, 4, 0, 0, 0, 15, 0, 0, 0, 23, 0, 0, 0, 19, 0, 0, 0, 13, 0, 0, 0, 12, 0, 0, 0, 2, 0, 0, 0, 20, 0, 0, 0, 14, 0, 0, 0, 22, 0, 0, 0, 9, 0, 0, 0, 6, 0, 0, 0, 1};
.global .align 8 .b8 keccak_round_constants[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};
.global .align 1 .b8 $str[23] = {72, 97, 115, 104, 32, 102, 111, 114, 32, 102, 105, 114, 115, 116, 32, 110, 111, 110, 99, 101, 58, 32};
.global .align 1 .b8 $str$1[5] = {37, 48, 50, 120};
.global .align 1 .b8 $str$2[2] = {10};

.visible .entry mine_patterns(
	.param .u64 mine_patterns_param_0,
	.param .u64 mine_patterns_param_1,
	.param .u64 mine_patterns_param_2,
	.param .u64 mine_patterns_param_3,
	.param .u64 mine_patterns_param_4,
	.param .u32 mine_patterns_param_5,
	.param .u32 mine_patterns_param_6,
	.param .u32 mine_patterns_param_7,
	.param .u32 mine_patterns_param_8
)
{
	.local .align 16 .b8 	__local_depot0[48];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<21>;
	.reg .b16 	%rs<12>;
	.reg .b32 	%r<255>;
	.reg .b64 	%rd<252>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd70, [mine_patterns_param_0];
	ld.param.u64 	%rd71, [mine_patterns_param_1];
	ld.param.u64 	%rd72, [mine_patterns_param_2];
	ld.param.u64 	%rd73, [mine_patterns_param_4];
	ld.param.u32 	%r9, [mine_patterns_param_5];
	ld.param.u32 	%r10, [mine_patterns_param_6];
	ld.param.u32 	%r11, [mine_patterns_param_7];
	ld.param.u32 	%r12, [mine_patterns_param_8];
	add.u64 	%rd1, %SPL, 0;
	mov.u32 	%r13, %ntid.x;
	mov.u32 	%r14, %ctaid.x;
	mov.u32 	%r15, %tid.x;
	mad.lo.s32 	%r1, %r14, %r13, %r15;
	setp.ge.s32 	%p2, %r1, %r12;
	@%p2 bra 	$L__BB0_19;

	cvt.s64.s32 	%rd2, %r11;
	setp.eq.s32 	%p3, %r11, 0;
	@%p3 bra 	$L__BB0_4;

	cvta.to.global.u64 	%rd3, %rd72;
	mov.u64 	%rd224, 0;

$L__BB0_3:
	add.s64 	%rd76, %rd3, %rd224;
	ld.global.u8 	%rs2, [%rd76];
	add.s64 	%rd77, %rd1, %rd224;
	st.local.u8 	[%rd77], %rs2;
	add.s64 	%rd224, %rd224, 1;
	setp.lt.u64 	%p4, %rd224, %rd2;
	@%p4 bra 	$L__BB0_3;

$L__BB0_4:
	add.u64 	%rd100, %SP, 32;
	add.u64 	%rd6, %SPL, 32;
	cvt.s64.s32 	%rd101, %r1;
	add.s64 	%rd102, %rd1, %rd2;
	mov.u64 	%rd226, 0;
	st.local.u8 	[%rd102], %rd101;
	shr.u64 	%rd103, %rd101, 56;
	st.local.u8 	[%rd102+7], %rd103;
	shr.u64 	%rd104, %rd101, 48;
	st.local.u8 	[%rd102+6], %rd104;
	shr.u64 	%rd105, %rd101, 40;
	st.local.u8 	[%rd102+5], %rd105;
	shr.u64 	%rd106, %rd101, 32;
	st.local.u8 	[%rd102+4], %rd106;
	shr.u64 	%rd107, %rd101, 24;
	st.local.u8 	[%rd102+3], %rd107;
	shr.u64 	%rd108, %rd101, 16;
	st.local.u8 	[%rd102+2], %rd108;
	shr.u64 	%rd109, %rd101, 8;
	st.local.u8 	[%rd102+1], %rd109;
	ld.local.v2.u64 	{%rd110, %rd249}, [%rd1];
	mov.u32 	%r252, 0;
	ld.local.v2.u64 	{%rd248, %rd247}, [%rd1+16];
	xor.b64  	%rd250, %rd110, 1;
	cvta.to.global.u64 	%rd12, %rd73;
	cvta.to.global.u64 	%rd13, %rd71;
	cvta.to.global.u64 	%rd14, %rd70;
	mov.u64 	%rd234, -9223372036854775808;
	mov.u64 	%rd225, keccak_round_constants;
	mov.u64 	%rd227, %rd226;
	mov.u64 	%rd228, %rd226;
	mov.u64 	%rd229, %rd226;
	mov.u64 	%rd230, %rd226;
	mov.u64 	%rd231, %rd226;
	mov.u64 	%rd232, %rd226;
	mov.u64 	%rd233, %rd226;
	mov.u64 	%rd235, %rd226;
	mov.u64 	%rd236, %rd226;
	mov.u64 	%rd237, %rd226;
	mov.u64 	%rd238, %rd226;
	mov.u64 	%rd239, %rd226;
	mov.u64 	%rd240, %rd226;
	mov.u64 	%rd241, %rd226;
	mov.u64 	%rd242, %rd226;
	mov.u64 	%rd243, %rd226;
	mov.u64 	%rd244, %rd226;
	mov.u64 	%rd245, %rd226;
	mov.u64 	%rd246, %rd226;

$L__BB0_5:
	xor.b64  	%rd115, %rd241, %rd246;
	xor.b64  	%rd116, %rd115, %rd236;
	xor.b64  	%rd117, %rd116, %rd231;
	xor.b64  	%rd118, %rd117, %rd226;
	cvt.u32.u64 	%r133, %rd249;
	cvt.u32.u64 	%r134, %rd244;
	xor.b32  	%r135, %r134, %r133;
	cvt.u32.u64 	%r136, %rd239;
	xor.b32  	%r137, %r135, %r136;
	cvt.u32.u64 	%r138, %rd234;
	xor.b32  	%r139, %r137, %r138;
	cvt.u32.u64 	%r140, %rd229;
	xor.b32  	%r18, %r139, %r140;
	mov.u32 	%r40, 1;
	// begin inline asm
	shf.l.wrap.b32 %r17, %r18, %r18, %r40;
	// end inline asm
	cvt.u64.u32 	%rd119, %r17;
	xor.b64  	%rd120, %rd118, %rd119;
	xor.b64  	%rd41, %rd120, %rd250;
	cvt.u32.u64 	%r141, %rd243;
	cvt.u32.u64 	%r142, %rd248;
	xor.b32  	%r143, %r141, %r142;
	cvt.u32.u64 	%r144, %rd238;
	xor.b32  	%r145, %r143, %r144;
	cvt.u32.u64 	%r146, %rd233;
	xor.b32  	%r147, %r145, %r146;
	cvt.u32.u64 	%r148, %rd228;
	xor.b32  	%r22, %r147, %r148;
	// begin inline asm
	shf.l.wrap.b32 %r21, %r22, %r22, %r40;
	// end inline asm
	cvt.u32.u64 	%r149, %rd247;
	cvt.u32.u64 	%r150, %rd242;
	xor.b32  	%r151, %r150, %r149;
	cvt.u32.u64 	%r152, %rd237;
	xor.b32  	%r153, %r151, %r152;
	cvt.u32.u64 	%r154, %rd232;
	xor.b32  	%r155, %r153, %r154;
	cvt.u32.u64 	%r156, %rd227;
	xor.b32  	%r26, %r155, %r156;
	// begin inline asm
	shf.l.wrap.b32 %r25, %r26, %r26, %r40;
	// end inline asm
	cvt.u32.u64 	%r31, %rd118;
	// begin inline asm
	shf.l.wrap.b32 %r29, %r31, %r31, %r40;
	// end inline asm
	cvt.u32.u64 	%r157, %rd250;
	cvt.u32.u64 	%r158, %rd245;
	xor.b32  	%r159, %r158, %r157;
	cvt.u32.u64 	%r160, %rd240;
	xor.b32  	%r161, %r159, %r160;
	cvt.u32.u64 	%r162, %rd235;
	xor.b32  	%r163, %r161, %r162;
	cvt.u32.u64 	%r164, %rd230;
	xor.b32  	%r34, %r163, %r164;
	// begin inline asm
	shf.l.wrap.b32 %r33, %r34, %r34, %r40;
	// end inline asm
	xor.b32  	%r165, %r34, %r21;
	xor.b32  	%r38, %r165, %r133;
	// begin inline asm
	shf.l.wrap.b32 %r37, %r38, %r38, %r40;
	// end inline asm
	cvt.u64.u32 	%rd121, %r37;
	cvt.u32.u64 	%r166, %rd120;
	xor.b32  	%r42, %r166, %r160;
	mov.u32 	%r44, 3;
	// begin inline asm
	shf.l.wrap.b32 %r41, %r42, %r42, %r44;
	// end inline asm
	cvt.u64.u32 	%rd122, %r41;
	xor.b32  	%r167, %r18, %r25;
	xor.b32  	%r46, %r167, %r141;
	mov.u32 	%r48, 6;
	// begin inline asm
	shf.l.wrap.b32 %r45, %r46, %r46, %r48;
	// end inline asm
	cvt.u64.u32 	%rd123, %r45;
	xor.b32  	%r50, %r165, %r136;
	mov.u32 	%r52, 10;
	// begin inline asm
	shf.l.wrap.b32 %r49, %r50, %r50, %r52;
	// end inline asm
	cvt.u64.u32 	%rd124, %r49;
	xor.b32  	%r54, %r167, %r146;
	mov.u32 	%r56, 15;
	// begin inline asm
	shf.l.wrap.b32 %r53, %r54, %r54, %r56;
	// end inline asm
	cvt.u64.u32 	%rd125, %r53;
	xor.b32  	%r168, %r22, %r29;
	xor.b32  	%r58, %r168, %r154;
	mov.u32 	%r60, 21;
	// begin inline asm
	shf.l.wrap.b32 %r57, %r58, %r58, %r60;
	// end inline asm
	cvt.u64.u32 	%rd126, %r57;
	xor.b32  	%r62, %r168, %r149;
	mov.u32 	%r64, 28;
	// begin inline asm
	shf.l.wrap.b32 %r61, %r62, %r62, %r64;
	// end inline asm
	cvt.u64.u32 	%rd127, %r61;
	xor.b32  	%r66, %r166, %r158;
	mov.u32 	%r68, 36;
	// begin inline asm
	shf.l.wrap.b32 %r65, %r66, %r66, %r68;
	// end inline asm
	cvt.u64.u32 	%rd128, %r65;
	xor.b32  	%r70, %r165, %r138;
	mov.u32 	%r72, 45;
	// begin inline asm
	shf.l.wrap.b32 %r69, %r70, %r70, %r72;
	// end inline asm
	cvt.u64.u32 	%rd129, %r69;
	xor.b32  	%r74, %r168, %r150;
	mov.u32 	%r76, 55;
	// begin inline asm
	shf.l.wrap.b32 %r73, %r74, %r74, %r76;
	// end inline asm
	cvt.u64.u32 	%rd130, %r73;
	xor.b32  	%r78, %r165, %r140;
	mov.u32 	%r80, 2;
	// begin inline asm
	shf.l.wrap.b32 %r77, %r78, %r78, %r80;
	// end inline asm
	cvt.u64.u32 	%rd131, %r77;
	cvt.u32.u64 	%r169, %rd226;
	xor.b32  	%r170, %r26, %r33;
	xor.b32  	%r82, %r170, %r169;
	mov.u32 	%r84, 14;
	// begin inline asm
	shf.l.wrap.b32 %r81, %r82, %r82, %r84;
	// end inline asm
	cvt.u64.u32 	%rd132, %r81;
	cvt.u32.u64 	%r171, %rd246;
	xor.b32  	%r86, %r170, %r171;
	mov.u32 	%r88, 27;
	// begin inline asm
	shf.l.wrap.b32 %r85, %r86, %r86, %r88;
	// end inline asm
	cvt.u64.u32 	%rd133, %r85;
	xor.b32  	%r90, %r166, %r162;
	mov.u32 	%r92, 41;
	// begin inline asm
	shf.l.wrap.b32 %r89, %r90, %r90, %r92;
	// end inline asm
	cvt.u64.u32 	%rd134, %r89;
	xor.b32  	%r94, %r168, %r156;
	mov.u32 	%r96, 56;
	// begin inline asm
	shf.l.wrap.b32 %r93, %r94, %r94, %r96;
	// end inline asm
	cvt.u64.u32 	%rd135, %r93;
	cvt.u32.u64 	%r172, %rd231;
	xor.b32  	%r98, %r170, %r172;
	mov.u32 	%r100, 8;
	// begin inline asm
	shf.l.wrap.b32 %r97, %r98, %r98, %r100;
	// end inline asm
	cvt.u64.u32 	%rd136, %r97;
	xor.b32  	%r102, %r168, %r152;
	mov.u32 	%r104, 25;
	// begin inline asm
	shf.l.wrap.b32 %r101, %r102, %r102, %r104;
	// end inline asm
	cvt.u64.u32 	%rd137, %r101;
	xor.b32  	%r106, %r167, %r144;
	mov.u32 	%r108, 43;
	// begin inline asm
	shf.l.wrap.b32 %r105, %r106, %r106, %r108;
	// end inline asm
	cvt.u64.u32 	%rd138, %r105;
	xor.b32  	%r110, %r167, %r142;
	mov.u32 	%r112, 62;
	// begin inline asm
	shf.l.wrap.b32 %r109, %r110, %r110, %r112;
	// end inline asm
	cvt.u64.u32 	%rd139, %r109;
	xor.b32  	%r114, %r166, %r164;
	mov.u32 	%r116, 18;
	// begin inline asm
	shf.l.wrap.b32 %r113, %r114, %r114, %r116;
	// end inline asm
	cvt.u64.u32 	%rd140, %r113;
	cvt.u32.u64 	%r173, %rd236;
	xor.b32  	%r118, %r170, %r173;
	mov.u32 	%r120, 39;
	// begin inline asm
	shf.l.wrap.b32 %r117, %r118, %r118, %r120;
	// end inline asm
	cvt.u64.u32 	%rd141, %r117;
	xor.b32  	%r122, %r167, %r148;
	mov.u32 	%r124, 61;
	// begin inline asm
	shf.l.wrap.b32 %r121, %r122, %r122, %r124;
	// end inline asm
	cvt.u64.u32 	%rd142, %r121;
	cvt.u32.u64 	%r174, %rd241;
	xor.b32  	%r126, %r170, %r174;
	mov.u32 	%r128, 20;
	// begin inline asm
	shf.l.wrap.b32 %r125, %r126, %r126, %r128;
	// end inline asm
	cvt.u64.u32 	%rd143, %r125;
	xor.b32  	%r130, %r165, %r134;
	mov.u32 	%r132, 44;
	// begin inline asm
	shf.l.wrap.b32 %r129, %r130, %r130, %r132;
	// end inline asm
	cvt.u64.u32 	%rd144, %r129;
	not.b64 	%rd145, %rd144;
	and.b64  	%rd146, %rd138, %rd145;
	xor.b64  	%rd147, %rd41, %rd146;
	not.b64 	%rd148, %rd138;
	and.b64  	%rd149, %rd126, %rd148;
	xor.b64  	%rd249, %rd149, %rd144;
	not.b64 	%rd150, %rd126;
	and.b64  	%rd151, %rd132, %rd150;
	xor.b64  	%rd248, %rd151, %rd138;
	not.b64 	%rd152, %rd132;
	and.b64  	%rd153, %rd41, %rd152;
	xor.b64  	%rd247, %rd153, %rd126;
	not.b64 	%rd154, %rd41;
	and.b64  	%rd155, %rd144, %rd154;
	xor.b64  	%rd246, %rd155, %rd132;
	not.b64 	%rd156, %rd143;
	and.b64  	%rd157, %rd122, %rd156;
	xor.b64  	%rd245, %rd157, %rd127;
	not.b64 	%rd158, %rd122;
	and.b64  	%rd159, %rd129, %rd158;
	xor.b64  	%rd244, %rd159, %rd143;
	not.b64 	%rd160, %rd129;
	and.b64  	%rd161, %rd142, %rd160;
	xor.b64  	%rd243, %rd161, %rd122;
	not.b64 	%rd162, %rd142;
	and.b64  	%rd163, %rd127, %rd162;
	xor.b64  	%rd242, %rd163, %rd129;
	not.b64 	%rd164, %rd127;
	and.b64  	%rd165, %rd143, %rd164;
	xor.b64  	%rd241, %rd165, %rd142;
	not.b64 	%rd166, %rd123;
	and.b64  	%rd167, %rd137, %rd166;
	xor.b64  	%rd240, %rd167, %rd121;
	not.b64 	%rd168, %rd137;
	and.b64  	%rd169, %rd136, %rd168;
	xor.b64  	%rd239, %rd169, %rd123;
	not.b64 	%rd170, %rd136;
	and.b64  	%rd171, %rd140, %rd170;
	xor.b64  	%rd238, %rd171, %rd137;
	not.b64 	%rd172, %rd140;
	and.b64  	%rd173, %rd121, %rd172;
	xor.b64  	%rd237, %rd173, %rd136;
	not.b64 	%rd174, %rd121;
	and.b64  	%rd175, %rd123, %rd174;
	xor.b64  	%rd236, %rd175, %rd140;
	not.b64 	%rd176, %rd128;
	and.b64  	%rd177, %rd124, %rd176;
	xor.b64  	%rd235, %rd177, %rd133;
	not.b64 	%rd178, %rd124;
	and.b64  	%rd179, %rd125, %rd178;
	xor.b64  	%rd234, %rd179, %rd128;
	not.b64 	%rd180, %rd125;
	and.b64  	%rd181, %rd135, %rd180;
	xor.b64  	%rd233, %rd181, %rd124;
	not.b64 	%rd182, %rd135;
	and.b64  	%rd183, %rd133, %rd182;
	xor.b64  	%rd232, %rd183, %rd125;
	not.b64 	%rd184, %rd133;
	and.b64  	%rd185, %rd128, %rd184;
	xor.b64  	%rd231, %rd185, %rd135;
	not.b64 	%rd186, %rd130;
	and.b64  	%rd187, %rd141, %rd186;
	xor.b64  	%rd230, %rd187, %rd139;
	not.b64 	%rd188, %rd141;
	and.b64  	%rd189, %rd134, %rd188;
	xor.b64  	%rd229, %rd189, %rd130;
	not.b64 	%rd190, %rd134;
	and.b64  	%rd191, %rd131, %rd190;
	xor.b64  	%rd228, %rd191, %rd141;
	not.b64 	%rd192, %rd131;
	and.b64  	%rd193, %rd139, %rd192;
	xor.b64  	%rd227, %rd193, %rd134;
	not.b64 	%rd194, %rd139;
	and.b64  	%rd195, %rd130, %rd194;
	xor.b64  	%rd226, %rd195, %rd131;
	ld.global.nc.u64 	%rd196, [%rd225];
	xor.b64  	%rd250, %rd147, %rd196;
	add.s64 	%rd225, %rd225, 8;
	add.s32 	%r252, %r252, 1;
	setp.ne.s32 	%p5, %r252, 24;
	@%p5 bra 	$L__BB0_5;

	st.local.v2.u64 	[%rd1], {%rd250, %rd249};
	st.local.v2.u64 	[%rd1+16], {%rd248, %rd247};
	setp.ne.s32 	%p6, %r1, 0;
	@%p6 bra 	$L__BB0_8;

	shr.u64 	%rd197, %rd41, 56;
	shr.u64 	%rd198, %rd41, 48;
	cvt.u32.u64 	%r175, %rd198;
	shr.u64 	%rd199, %rd41, 40;
	cvt.u32.u64 	%r176, %rd199;
	shr.u64 	%rd200, %rd41, 32;
	cvt.u32.u64 	%r177, %rd200;
	cvt.u32.u64 	%r178, %rd247;
	shr.u32 	%r179, %r178, 16;
	shr.u32 	%r180, %r178, 8;
	shr.u64 	%rd201, %rd248, 24;
	cvt.u32.u64 	%r181, %rd248;
	shr.u32 	%r182, %r181, 16;
	shr.u32 	%r183, %r181, 8;
	shr.u64 	%rd202, %rd249, 24;
	cvt.u32.u64 	%r184, %rd249;
	shr.u32 	%r185, %r184, 16;
	shr.u32 	%r186, %r184, 8;
	shr.u64 	%rd203, %rd250, 56;
	shr.u64 	%rd204, %rd250, 48;
	cvt.u32.u64 	%r187, %rd204;
	shr.u64 	%rd205, %rd250, 40;
	cvt.u32.u64 	%r188, %rd205;
	shr.u64 	%rd206, %rd250, 32;
	cvt.u32.u64 	%r189, %rd206;
	cvt.u32.u64 	%r190, %rd250;
	shr.u32 	%r191, %r190, 16;
	shr.u32 	%r192, %r190, 8;
	mov.u64 	%rd207, $str;
	cvta.global.u64 	%rd208, %rd207;
	mov.u32 	%r193, 0;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd208;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r194, [retval0+0];
	} // callseq 0
	and.b32  	%r195, %r190, 255;
	st.local.u32 	[%rd6], %r195;
	mov.u64 	%rd209, $str$1;
	cvta.global.u64 	%rd210, %rd209;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r196, [retval0+0];
	} // callseq 1
	and.b32  	%r197, %r192, 255;
	st.local.u32 	[%rd6], %r197;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r198, [retval0+0];
	} // callseq 2
	and.b32  	%r199, %r191, 255;
	st.local.u32 	[%rd6], %r199;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r200, [retval0+0];
	} // callseq 3
	shr.u32 	%r201, %r190, 24;
	st.local.u32 	[%rd6], %r201;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r202, [retval0+0];
	} // callseq 4
	and.b32  	%r203, %r189, 255;
	st.local.u32 	[%rd6], %r203;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r204, [retval0+0];
	} // callseq 5
	and.b32  	%r205, %r188, 255;
	st.local.u32 	[%rd6], %r205;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r206, [retval0+0];
	} // callseq 6
	and.b32  	%r207, %r187, 255;
	st.local.u32 	[%rd6], %r207;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r208, [retval0+0];
	} // callseq 7
	st.local.u32 	[%rd6], %rd203;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r209, [retval0+0];
	} // callseq 8
	and.b32  	%r210, %r184, 255;
	st.local.u32 	[%rd6], %r210;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r211, [retval0+0];
	} // callseq 9
	and.b32  	%r212, %r186, 255;
	st.local.u32 	[%rd6], %r212;
	{ // callseq 10, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r213, [retval0+0];
	} // callseq 10
	and.b32  	%r214, %r185, 255;
	st.local.u32 	[%rd6], %r214;
	{ // callseq 11, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r215, [retval0+0];
	} // callseq 11
	st.local.u32 	[%rd6], %rd202;
	{ // callseq 12, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r216, [retval0+0];
	} // callseq 12
	st.local.u32 	[%rd6], %r193;
	{ // callseq 13, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r217, [retval0+0];
	} // callseq 13
	st.local.u32 	[%rd6], %r193;
	{ // callseq 14, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r218, [retval0+0];
	} // callseq 14
	st.local.u32 	[%rd6], %r193;
	{ // callseq 15, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r219, [retval0+0];
	} // callseq 15
	st.local.u32 	[%rd6], %r193;
	{ // callseq 16, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r220, [retval0+0];
	} // callseq 16
	and.b32  	%r221, %r181, 255;
	st.local.u32 	[%rd6], %r221;
	{ // callseq 17, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r222, [retval0+0];
	} // callseq 17
	and.b32  	%r223, %r183, 255;
	st.local.u32 	[%rd6], %r223;
	{ // callseq 18, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r224, [retval0+0];
	} // callseq 18
	and.b32  	%r225, %r182, 255;
	st.local.u32 	[%rd6], %r225;
	{ // callseq 19, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r226, [retval0+0];
	} // callseq 19
	st.local.u32 	[%rd6], %rd201;
	{ // callseq 20, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r227, [retval0+0];
	} // callseq 20
	st.local.u32 	[%rd6], %r193;
	{ // callseq 21, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r228, [retval0+0];
	} // callseq 21
	st.local.u32 	[%rd6], %r193;
	{ // callseq 22, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r229, [retval0+0];
	} // callseq 22
	st.local.u32 	[%rd6], %r193;
	{ // callseq 23, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r230, [retval0+0];
	} // callseq 23
	st.local.u32 	[%rd6], %r193;
	{ // callseq 24, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r231, [retval0+0];
	} // callseq 24
	and.b32  	%r232, %r178, 255;
	st.local.u32 	[%rd6], %r232;
	{ // callseq 25, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r233, [retval0+0];
	} // callseq 25
	and.b32  	%r234, %r180, 255;
	st.local.u32 	[%rd6], %r234;
	{ // callseq 26, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r235, [retval0+0];
	} // callseq 26
	and.b32  	%r236, %r179, 255;
	st.local.u32 	[%rd6], %r236;
	{ // callseq 27, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r237, [retval0+0];
	} // callseq 27
	shr.u32 	%r238, %r178, 24;
	st.local.u32 	[%rd6], %r238;
	{ // callseq 28, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r239, [retval0+0];
	} // callseq 28
	and.b32  	%r240, %r177, 255;
	st.local.u32 	[%rd6], %r240;
	{ // callseq 29, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r241, [retval0+0];
	} // callseq 29
	and.b32  	%r242, %r176, 255;
	st.local.u32 	[%rd6], %r242;
	{ // callseq 30, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r243, [retval0+0];
	} // callseq 30
	and.b32  	%r244, %r175, 255;
	st.local.u32 	[%rd6], %r244;
	{ // callseq 31, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r245, [retval0+0];
	} // callseq 31
	st.local.u32 	[%rd6], %rd197;
	{ // callseq 32, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd210;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd100;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r246, [retval0+0];
	} // callseq 32
	mov.u64 	%rd212, $str$2;
	cvta.global.u64 	%rd213, %rd212;
	{ // callseq 33, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd213;
	.param .b64 param1;
	st.param.b64 	[param1+0], 0;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r247, [retval0+0];
	} // callseq 33

$L__BB0_8:
	setp.lt.s32 	%p7, %r9, 1;
	mov.u16 	%rs3, 1;
	mov.u16 	%rs11, %rs3;
	@%p7 bra 	$L__BB0_12;

	mov.u32 	%r253, 0;

$L__BB0_10:
	cvt.s64.s32 	%rd214, %r253;
	add.s64 	%rd215, %rd1, %rd214;
	add.s64 	%rd216, %rd14, %rd214;
	ld.global.u8 	%rs5, [%rd216];
	ld.local.u8 	%rs6, [%rd215];
	setp.ne.s16 	%p8, %rs6, %rs5;
	add.s32 	%r253, %r253, 1;
	mov.u16 	%rs11, 0;
	@%p8 bra 	$L__BB0_12;

	setp.lt.s32 	%p9, %r253, %r9;
	mov.u16 	%rs11, %rs3;
	@%p9 bra 	$L__BB0_10;

$L__BB0_12:
	setp.lt.s32 	%p11, %r10, 1;
	mov.pred 	%p10, -1;
	mov.pred 	%p20, %p10;
	@%p11 bra 	$L__BB0_16;

	mov.u32 	%r250, 32;
	sub.s32 	%r6, %r250, %r10;
	mov.u32 	%r254, 0;

$L__BB0_14:
	add.s32 	%r251, %r6, %r254;
	cvt.s64.s32 	%rd217, %r251;
	add.s64 	%rd218, %rd1, %rd217;
	cvt.s64.s32 	%rd219, %r254;
	add.s64 	%rd220, %rd13, %rd219;
	ld.global.u8 	%rs8, [%rd220];
	ld.local.u8 	%rs9, [%rd218];
	setp.ne.s16 	%p13, %rs9, %rs8;
	add.s32 	%r254, %r254, 1;
	mov.pred 	%p20, 0;
	@%p13 bra 	$L__BB0_16;

	setp.lt.s32 	%p15, %r254, %r10;
	mov.pred 	%p20, %p10;
	@%p15 bra 	$L__BB0_14;

$L__BB0_16:
	setp.eq.s16 	%p16, %rs11, 0;
	not.pred 	%p17, %p20;
	or.pred  	%p18, %p16, %p17;
	@%p18 bra 	$L__BB0_19;

	mov.u64 	%rd251, 0;

$L__BB0_18:
	add.s64 	%rd222, %rd1, %rd251;
	ld.local.u8 	%rs10, [%rd222];
	add.s64 	%rd223, %rd12, %rd251;
	st.global.u8 	[%rd223], %rs10;
	add.s64 	%rd251, %rd251, 1;
	setp.lt.u64 	%p19, %rd251, 32;
	@%p19 bra 	$L__BB0_18;

$L__BB0_19:
	ret;

}

